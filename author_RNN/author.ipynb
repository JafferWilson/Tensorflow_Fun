{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# playing around with lstms and text\n",
    "\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pg18500.txt\n"
     ]
    }
   ],
   "source": [
    "# url = 'http://constitution.org/'\n",
    "url = 'http://www.gutenberg.org/cache/epub/18500/'\n",
    "def download(filename):\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  return filename\n",
    "\n",
    "filename = download('pg18500.txt')\n",
    "# filename = download('usdeclar.txt')\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 1847013\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = tf.compat.as_str(f.read())\n",
    "    return data\n",
    "\n",
    "def clean(arr):\n",
    "    str = ''.join([c for c in arr if c.isalpha() or c == ' ' or c == '\\n'])\n",
    "    str = re.sub(r\"[ \\n]{2,}\", \" \", str)\n",
    "    str = re.sub(r\"[\\n]\", \" \", str)\n",
    "    # remove empty tokens\n",
    "    cleaned = [tok for tok in str.split(' ') if tok]\n",
    "    return ' '.join(cleaned), cleaned\n",
    "\n",
    "text, text_list = clean(read_data(filename))\n",
    "#text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1846013 hroughout the book No change has been made in this The relative indentations of Poems Epitaphs and Songs \n",
      "1000 The Project Gutenberg EBook of The Complete Works of Robert Burns Containing his Poems Songs and Correspo\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:105])\n",
    "print(valid_size, valid_text[:105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348008\n"
     ]
    }
   ],
   "source": [
    "print(len(text_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "29983\n"
     ]
    }
   ],
   "source": [
    "# find map word -> id\n",
    "\n",
    "words = dict()\n",
    "ids = dict()\n",
    "index = 0\n",
    "for word in text_list:\n",
    "    if word not in words:\n",
    "        words[word] = index\n",
    "        ids[index] = word\n",
    "        index = index + 1\n",
    "# print(words['Gutenberg'])\n",
    "# print(words['Works'])\n",
    "print(ids[0])\n",
    "\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# generate batches\n",
    "\n",
    "batch_size=15\n",
    "num_unrollings=6\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocab_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, words[self._text[self._cursor[b]]]] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  words back into its (most likely) character representation.\"\"\"\n",
    "  return [ids[c] for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  print(np.array(batches).shape)\n",
    "  print('vocab size:', vocab_size)\n",
    "  for b in batches:\n",
    "    s = [' '.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 15, 29983)\n",
      "105\n"
     ]
    }
   ],
   "source": [
    "batch_tester = BatchGenerator(text_list, batch_size, num_unrollings)\n",
    "batches = batch_tester.next()\n",
    "batches_np = np.array(batches)\n",
    "print(batches_np.shape)\n",
    "print(np.count_nonzero(batches_np))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 15, 29983)\n",
      "vocab size: 29983\n",
      "[' Complete Works of Robert Burns Containing his', ' that by his skill in rearing sheep', ' unison between the pair An made the', ' your duty duly morn and night Lest', ' was first printed by Stewart in The', ' warmer heart death neer made cold III', ' chair shall fa He is the king', ' liberty and independence which I threw into', ' dear Sir Yours ROBT BURNESS FOOTNOTES Footnote', ' rhymes The first of my poetic offspring', ' was talked of We got a song', ' the one side and my fiddlecase on', ' other In the honest enthusiasm with which', ' me long ago but over and above', ' they disguise as like wretched old age']\n",
      "(7, 15, 29983)\n",
      "105\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "new_batches = batch_tester.next()\n",
    "new_np = np.array(new_batches)\n",
    "\n",
    "print(batches2string(new_batches))\n",
    "\n",
    "print(new_np.shape)\n",
    "print(np.count_nonzero(new_np))\n",
    "\n",
    "# verify batches are different\n",
    "print(np.array_equal(batches_np, new_np))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# single layer RNN\n",
    "\n",
    "batch_size = 10\n",
    "num_unrollings = 12\n",
    "num_cells = num_unrollings - 1\n",
    "# cell output size\n",
    "state_size = 30\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# trying without embeddings (only word ids)\n",
    "\n",
    "# 1-hot\n",
    "# inputs = list length num_inputs of [batch_size, vocab_size]\n",
    "inputs = [tf.placeholder(tf.float32, shape=[None, vocab_size]) for _ in range(num_cells)]\n",
    "labels = [tf.placeholder(tf.float32, shape=[None, vocab_size]) for _ in range(num_cells)]\n",
    "\n",
    "# 2-tuple initial state\n",
    "state = tf.placeholder(tf.float32, shape=[None, state_size])\n",
    "hidden_state = tf.placeholder(tf.float32, shape=[None, state_size])\n",
    "\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(state_size, state_is_tuple=True)\n",
    "\n",
    "# outputs list of [batch_size, output_size]\n",
    "outputs, final_state = tf.contrib.rnn.static_rnn(cell, inputs, (state, hidden_state), scope=\"tf.contrib.rnn.BasicLSTMCell\")\n",
    "\n",
    "stddev1 = np.sqrt(2.0 / state_size * vocab_size)\n",
    "w = tf.Variable(tf.truncated_normal([state_size, vocab_size], stddev=stddev1))\n",
    "stddev2 = np.sqrt(2.0 / vocab_size)\n",
    "b = tf.Variable(tf.truncated_normal([vocab_size], stddev=stddev2))\n",
    "\n",
    "losses = []\n",
    "predictions = []\n",
    "for i in range(num_cells):\n",
    "    logits = tf.matmul(outputs[i], w) + b\n",
    "    predictions.append(tf.argmax(logits, 1))\n",
    "    losses.append(tf.nn.softmax_cross_entropy_with_logits(labels=labels[i], logits=logits))\n",
    "\n",
    "# losses = [tf.nn.softmax_cross_entropy_with_logits(labels=labels[i], \n",
    "#             logits=tf.matmul(outputs[i], w) + b) for i in range(num_cells)]\n",
    "\n",
    "loss = tf.reduce_mean(losses)\n",
    "optimizer = tf.train.AdamOptimizer(1e-3).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 10.520134\n",
      "Loss: 10.520571\n",
      "(11, 10, 29983)\n",
      "Input: money paid him in drink and paid him in verse With\n",
      "Labels: paid him in drink and paid him in verse With Pegasus\n",
      "Predict: cutting billetdoux excuses chasd namesake portraits portraits excuses stripe Ghaist Sonnet\n",
      "Loss: 10.296631\n",
      "Loss: 10.362860\n",
      "(11, 10, 29983)\n",
      "Input: very devil He though two years younger has completely mastered his\n",
      "Labels: devil He though two years younger has completely mastered his brother\n",
      "Predict: PUBLIC fiddlers Wrongs stripe attour poverty Gully censure Gully hearthstone advertised\n",
      "Loss: 10.260087\n",
      "Loss: 10.286766\n",
      "(11, 10, 29983)\n",
      "Input: to my mou gied a sten For thrice I drew ane\n",
      "Labels: my mou gied a sten For thrice I drew ane without\n",
      "Predict: the shear shear stroll parched Vengeance parched Vengeance Symon Vengeance Gully\n",
      "Loss: 10.162065\n",
      "Loss: 10.269129\n",
      "(11, 10, 29983)\n",
      "Input: first order When I grow richer I will write to you\n",
      "Labels: order When I grow richer I will write to you on\n",
      "Predict: flowrreviving Arianism seasonable proneness Clinkumbell highlyvalued Swatch delicacies delicacies delicacies delicacies\n",
      "Loss: 10.020248\n",
      "Loss: 10.035515\n",
      "(11, 10, 29983)\n",
      "Input: for the next week at Aberdeen the next at Edinburgh The\n",
      "Labels: the next week at Aberdeen the next at Edinburgh The sheet\n",
      "Predict: a invaded sprush vivacity unceremonious stripe Abram unceremonious unceremonious stripe mitigated\n",
      "Loss: 9.911410\n",
      "Loss: 10.243709\n",
      "(11, 10, 29983)\n",
      "Input: equal frankness My wish is not to stand aloof the uncomplying\n",
      "Labels: frankness My wish is not to stand aloof the uncomplying bigot\n",
      "Predict: immemorial ingine beganwrote lowered ridge stripe stripe a a admonishd PREFERENCE\n",
      "Loss: 10.139161\n",
      "Loss: 10.271307\n",
      "(11, 10, 29983)\n",
      "Input: to the end But who can with fate and quartbumpers contend\n",
      "Labels: the end But who can with fate and quartbumpers contend Though\n",
      "Predict: the of swanwhite mair paste glances rudely remarks remarks debated The\n",
      "Loss: 10.002011\n",
      "Loss: 10.151351\n",
      "(11, 10, 29983)\n",
      "Input: laid th accomplishd Burnet low Thy form and mind sweet maid\n",
      "Labels: th accomplishd Burnet low Thy form and mind sweet maid can\n",
      "Predict: in suffrage guinea leased e myselfthat Gretnagreen sunshine WOOD leased Ajaxs\n",
      "Loss: 9.947992\n",
      "Loss: 10.057460\n",
      "(11, 10, 29983)\n",
      "Input: born And hell be hame thats far awa And my young\n",
      "Labels: And hell be hame thats far awa And my young babie\n",
      "Predict: Sir spritual many pokes JESSIE aim lord aim lord scar cornsowing\n",
      "Loss: 10.079905\n",
      "Loss: 10.017117\n",
      "(11, 10, 29983)\n",
      "Input: VII To catch dame Fortunes golden smile Assiduous wait upon her\n",
      "Labels: To catch dame Fortunes golden smile Assiduous wait upon her And\n",
      "Predict: Monarch Weak stripe Halfwaukend comforter frustrate holmland holmland ben wyte phlebotomy\n",
      "Loss: 10.139942\n",
      "Loss: 9.854655\n",
      "(11, 10, 29983)\n",
      "Input: condition of an itinerant beggarCURRIE III TO MR JAMES BURNESS WRITER\n",
      "Labels: of an itinerant beggarCURRIE III TO MR JAMES BURNESS WRITER MONTROSE\n",
      "Predict: scornis the the wyld Brings ruler MR THOMSON regarded i thunderstruck\n",
      "Loss: 10.086550\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-768b7566dd90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# indices of words i.e. [batch_size]\n",
    "def get_words(indices):\n",
    "    l = []\n",
    "    for i in indices:\n",
    "        l.append(ids[i])\n",
    "    return l\n",
    "\n",
    "# batch_size = 10\n",
    "# num_unrollings = 12\n",
    "# num_cells = num_unrollings - 1\n",
    "\n",
    "# train\n",
    "num_iterations = 30000\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "generator = BatchGenerator(text_list, batch_size, num_unrollings)\n",
    "for i in range(num_iterations):\n",
    "    batches = generator.next()\n",
    "    feed_dict = dict()\n",
    "    for j in range(num_cells):\n",
    "        feed_dict[inputs[j]] = batches[j]\n",
    "        feed_dict[labels[j]] = batches[j + 1]\n",
    "    feed_dict[state] = np.zeros((batch_size, state_size))\n",
    "    feed_dict[hidden_state] = np.zeros((batch_size, state_size))\n",
    "    \n",
    "    _, l, pred, labs, input_np = sess.run([optimizer, loss, predictions, labels, inputs], feed_dict=feed_dict)\n",
    "    if i % 10 == 0 and i != 0:\n",
    "        print('Loss: %f' % l)\n",
    "    if i % 20 == 0 and i != 0:\n",
    "        rand_ind = random.randint(0, batch_size - 1)\n",
    "        input_np = np.array(input_np)\n",
    "        labs = np.array(labs)\n",
    "        pred = np.array(pred)\n",
    "        print(input_np.shape)\n",
    "        print('Input:', ' '.join(get_words(np.argmax(input_np[:, rand_ind], 1))))\n",
    "        print('Labels:', ' '.join(get_words(np.argmax(labs[:, rand_ind], 1))))\n",
    "        print('Predict:', ' '.join(get_words(pred[:, rand_ind])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of the Adopted in Congress July The Unanimous Declaration of the\n",
      "the people in Congress July The Unanimous Declaration of the Thirteen\n",
      "most to the July The Unanimous Declaration of the Thirteen United\n",
      "humble valuable Creator The Unanimous Declaration of the Thirteen United States\n",
      "terms becomes certain laws laws of Thirteen Thirteen United States of\n",
      "our our places form form nature United United States of America\n",
      "most British unusual happiness as and and States of America in\n",
      "humble Crown power encourage encourage endowed them them America in General\n",
      "terms and all their their by guards guards created created Congress\n",
      "our foreign cases high high repeated for for power equal their\n",
      "most mercenaries full seas seas bear bear the giving independent this\n",
      "humble to to to to be arms arms his convulsions totally\n",
      "terms terms our redress redress formidable against against against within judiciary\n",
      "our our repeated in in trade bodies us us He He\n",
      "most British petitions petitions death all all all all has has\n",
      "humble Crown have have legislative with assume migration migration endeavored endeavored\n",
      "terms and that all been all among hither hither connection to\n",
      "our foreign States political deaf political powers swarms swarms and us\n",
      "most mercenaries of connection the connection the of of among duty\n",
      "humble to to is so between us us us us to\n",
      "terms terms our redress all Unanimous He He all all all\n",
      "our our repeated in out state is is sacred political political\n",
      "most British petitions petitions his of undistinguished undistinguished honor instituted connection\n",
      "humble Crown have have been governors separation destruction be of of\n",
      "terms and that all all to magnanimity all support have have\n",
      "our foreign States political political political and and from been been\n",
      "most mercenaries of connection connection connection circumstances circumstances remaining remaining formidable\n",
      "humble to to is between between us us us emigration emigration\n",
      "terms terms our redress them them acts He all settlement settlement\n",
      "our our repeated in out so and against political erected here\n",
      "most British petitions petitions his his his his connection a beyond\n",
      "humble Crown have have been governors governors been laws of fortunes\n",
      "terms and that all all to to commit commit connected tyranny\n",
      "our foreign States political political political powers and on enemies with\n",
      "most mercenaries of connection connection connection of circumstances the in endeavored\n",
      "humble to to is between between us us us us his\n",
      "terms terms our redress them them acts He all all all\n",
      "our our repeated in out so and against political political political\n",
      "most British petitions petitions his his his his connection connection connection\n",
      "humble Crown have have been governors governors been laws between invasions\n",
      "terms and that all all to to commit commit ages and\n",
      "our foreign States political political political powers and on and connection\n",
      "most mercenaries of connection connection connection of circumstances the eat them\n",
      "humble to to is between between us us us contract mercenaries\n",
      "terms terms our redress them them acts He all alliances shall\n",
      "our our repeated in out so and against political establish seem\n",
      "most British petitions petitions his his his his connection commerce most\n",
      "humble Crown have have been governors governors been laws armies his\n",
      "terms and that all all to to commit commit foreign foreign\n",
      "our foreign States political political political powers and on He mercenaries\n"
     ]
    }
   ],
   "source": [
    "# generate new stuff\n",
    "\n",
    "num_gen = 50\n",
    "\n",
    "# take 1st part of it\n",
    "test_generator = BatchGenerator(text_list, 1, num_unrollings)\n",
    "next_in = test_generator.next()\n",
    "for i in range(num_gen):\n",
    "    feed_dict = dict()\n",
    "    for j in range(num_cells):\n",
    "        feed_dict[inputs[j]] = next_in[j]\n",
    "        # ignored\n",
    "        feed_dict[labels[j]] = next_in[j]\n",
    "    # clears previous state\n",
    "    feed_dict[state] = np.zeros((1, state_size))\n",
    "    feed_dict[hidden_state] = np.zeros((1, state_size))\n",
    "    pred = sess.run(predictions, feed_dict=feed_dict)\n",
    "    print(' '.join(get_words(np.array(pred).reshape((-1)))))\n",
    "    next_in = np.zeros(shape=[num_cells, 1, vocab_size])\n",
    "    for i in range(num_cells):\n",
    "        next_in[i, 0, pred[i]] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# trying multi layer RNN\n",
    "\n",
    "batch_size = 10\n",
    "num_unrollings = 12\n",
    "num_cells = num_unrollings - 1\n",
    "layers = 3\n",
    "# cell output size\n",
    "state_size = 30\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# trying without embeddings (only word ids)\n",
    "\n",
    "# 1-hot\n",
    "# inputs = list length num_inputs of [batch_size, vocab_size]\n",
    "inputs = [tf.placeholder(tf.float32, shape=[None, vocab_size]) for _ in range(num_cells)]\n",
    "labels = [tf.placeholder(tf.float32, shape=[None, vocab_size]) for _ in range(num_cells)]\n",
    "\n",
    "# init state = [layers, 2, None, state_size]\n",
    "states = tf.placeholder(tf.float32, shape=[layers, 2, None, state_size])\n",
    "\n",
    "# list of tensors\n",
    "initial_state = [ (states[i][0], states[i][1]) for i in range(layers)]\n",
    "\n",
    "cell = tf.contrib.rnn.LSTMCell(state_size)\n",
    "cell = tf.contrib.rnn.MultiRNNCell([cell] * layers, state_is_tuple=True)\n",
    "\n",
    "outputs, final_state = tf.contrib.rnn.static_rnn(cell, inputs, initial_state=initial_state)\n",
    "\n",
    "stddev1 = np.sqrt(2.0 / state_size * vocab_size)\n",
    "w = tf.Variable(tf.truncated_normal([state_size, vocab_size], stddev=stddev1))\n",
    "stddev2 = np.sqrt(2.0 / vocab_size)\n",
    "b = tf.Variable(tf.truncated_normal([vocab_size], stddev=stddev2))\n",
    "\n",
    "losses = []\n",
    "predictions = []\n",
    "for i in range(num_cells):\n",
    "    logits = tf.matmul(outputs[i], w) + b\n",
    "    predictions.append(tf.argmax(logits, 1))\n",
    "    losses.append(tf.nn.softmax_cross_entropy_with_logits(labels=labels[i], logits=logits))\n",
    "\n",
    "# losses = [tf.nn.softmax_cross_entropy_with_logits(labels=labels[i], \n",
    "#             logits=tf.matmul(outputs[i], w) + b) for i in range(num_cells)]\n",
    "\n",
    "loss = tf.reduce_mean(losses)\n",
    "optimizer = tf.train.AdamOptimizer(1e-3).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 10.275769\n",
      "Loss: 10.133775\n",
      "(11, 10, 29983)\n",
      "Input: illadvised people had uncoupled the merciless pack of the law at\n",
      "Labels: people had uncoupled the merciless pack of the law at my\n",
      "Predict: the spoonful spoonful spoonful spoonful spoonful spoonful spoonful spoonful spoonful MEN\n",
      "Loss: 10.136617\n",
      "Loss: 10.120363\n",
      "(11, 10, 29983)\n",
      "Input: Fingal The god of the bottle sends down from his hall\n",
      "Labels: The god of the bottle sends down from his hall This\n",
      "Predict: the spoonful spoonful spoonful spoonful spoonful spoonful spoonful spoonful spoonful spoonful\n",
      "Loss: 10.116810\n",
      "Loss: 10.119184\n",
      "(11, 10, 29983)\n",
      "Input: he was singing the tears down came Therell never be peace\n",
      "Labels: was singing the tears down came Therell never be peace till\n",
      "Predict: the the the the the the the MEN MEN MEN MEN\n",
      "Loss: 10.076747\n",
      "Loss: 10.014558\n",
      "(11, 10, 29983)\n",
      "Input: them with life and beauty His songs have all the beauties\n",
      "Labels: with life and beauty His songs have all the beauties and\n",
      "Predict: the the the the MEN MEN MEN MEN MEN MEN MEN\n",
      "Loss: 10.092793\n",
      "Loss: 9.920844\n",
      "(11, 10, 29983)\n",
      "Input: cared I for fishingtowns or fertile carses I slept at the\n",
      "Labels: I for fishingtowns or fertile carses I slept at the famous\n",
      "Predict: the the the the the the the the the the the\n",
      "Loss: 9.981118\n",
      "Loss: 10.031064\n",
      "(11, 10, 29983)\n",
      "Input: such a serious manner which to tell you the truth has\n",
      "Labels: a serious manner which to tell you the truth has made\n",
      "Predict: the the the the the the the the the degree degree\n",
      "Loss: 9.957180\n",
      "Loss: 9.965518\n",
      "(11, 10, 29983)\n",
      "Input: a villains years CXXIX A VISION This Vision of Liberty descended\n",
      "Labels: villains years CXXIX A VISION This Vision of Liberty descended on\n",
      "Predict: the the the the the the the the the the the\n",
      "Loss: 9.863587\n",
      "Loss: 9.832288\n",
      "(11, 10, 29983)\n",
      "Input: such circumstances I can assure you my dear from my own\n",
      "Labels: circumstances I can assure you my dear from my own feelings\n",
      "Predict: the the the the I I I I I I I\n",
      "Loss: 9.814404\n",
      "Loss: 9.862645\n",
      "(11, 10, 29983)\n",
      "Input: muir I have followed my Willie be the best R B\n",
      "Labels: I have followed my Willie be the best R B CCL\n",
      "Predict: the the I I I I I I I I I\n",
      "Loss: 9.834097\n",
      "Loss: 9.854807\n",
      "(11, 10, 29983)\n",
      "Input: fiveandforty years thegither An no forgetting wabster Charlie Im tauld he\n",
      "Labels: years thegither An no forgetting wabster Charlie Im tauld he offers\n",
      "Predict: the the the feeling feeling feeling feeling feeling feeling feeling feeling\n",
      "Loss: 9.805093\n",
      "Loss: 9.757434\n",
      "(11, 10, 29983)\n",
      "Input: tempest flyin Tirlin the kirks Whiles in the human bosom pryin\n",
      "Labels: flyin Tirlin the kirks Whiles in the human bosom pryin Unseen\n",
      "Predict: the the the the the the the the the the the\n",
      "Loss: 9.740280\n",
      "Loss: 9.694689\n",
      "(11, 10, 29983)\n",
      "Input: Arrangements for a trip in Galloway CCXCVI To Mrs Dunlop Threatened\n",
      "Labels: for a trip in Galloway CCXCVI To Mrs Dunlop Threatened with\n",
      "Predict: the the the the the the the the the the the\n",
      "Loss: 9.567091\n",
      "Loss: 9.656248\n",
      "(11, 10, 29983)\n",
      "Input: rest of the words GRAMACHREE The song of Gramachree was composed\n",
      "Labels: of the words GRAMACHREE The song of Gramachree was composed by\n",
      "Predict: the the the the the the the the the the the\n",
      "Loss: 9.556791\n",
      "Loss: 9.665282\n",
      "(11, 10, 29983)\n",
      "Input: The rest of the song as well as those songs in\n",
      "Labels: rest of the song as well as those songs in the\n",
      "Predict: the the the the the the the the the the the\n",
      "Loss: 9.360930\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-d0b89a9b31a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m25\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# multi layer\n",
    "# indices of words i.e. [batch_size]\n",
    "def get_words(indices):\n",
    "    l = []\n",
    "    for i in indices:\n",
    "        l.append(ids[i])\n",
    "    return l\n",
    "\n",
    "# batch_size = 10\n",
    "# num_unrollings = 12\n",
    "# num_cells = num_unrollings - 1\n",
    "\n",
    "# train\n",
    "num_iterations = 30000\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "generator = BatchGenerator(text_list, batch_size, num_unrollings)\n",
    "for i in range(num_iterations):\n",
    "    batches = generator.next()\n",
    "    feed_dict = dict()\n",
    "    for j in range(num_cells):\n",
    "        feed_dict[inputs[j]] = batches[j]\n",
    "        feed_dict[labels[j]] = batches[j + 1]\n",
    "    feed_dict[states] = np.zeros((layers, 2, batch_size, state_size))\n",
    "    \n",
    "    _, l, pred, labs, input_np = sess.run([optimizer, loss, predictions, labels, inputs], feed_dict=feed_dict)\n",
    "    if i % 25 == 0 and i != 0:\n",
    "        print('Loss: %f' % l)\n",
    "    if i % 50 == 0 and i != 0:\n",
    "        rand_ind = random.randint(0, batch_size - 1)\n",
    "        input_np = np.array(input_np)\n",
    "        labs = np.array(labs)\n",
    "        pred = np.array(pred)\n",
    "        print(input_np.shape)\n",
    "        print('Input:', ' '.join(get_words(np.argmax(input_np[:, rand_ind], 1))))\n",
    "        print('Labels:', ' '.join(get_words(np.argmax(labs[:, rand_ind], 1))))\n",
    "        print('Predict:', ' '.join(get_words(pred[:, rand_ind])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
